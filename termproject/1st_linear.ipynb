{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gBeHAYEa8_H"
      },
      "source": [
        "# **1. Import Module & Functions**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FGNCLr5nRdKS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import multiprocessing\n",
        "import copy\n",
        "import pickle\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from time import time, sleep, mktime\n",
        "from matplotlib import font_manager as fm, rc, rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array, nan, random as rnd, where\n",
        "import pandas as pd\n",
        "from pandas import DataFrame as dataframe, Series as series, isna, read_csv\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "from sklearn import preprocessing as prep\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split as tts, GridSearchCV as GridTuner, StratifiedKFold, KFold\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn import linear_model as lm\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as qda\n",
        "from sklearn import svm\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn import neighbors as knn\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# ===== import functions =====\n",
        "import sys\n",
        "sys.path.append(\"projects/DA_Platform\")\n",
        "#from DA_v5 import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BU_pU9LT0LPT"
      },
      "outputs": [],
      "source": [
        "# global setting\n",
        "warnings.filterwarnings(action='ignore')\n",
        "rcParams['axes.unicode_minus'] = False\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "folder_path = \"projects/dacon_stockprediction/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ut512M-bSvKM"
      },
      "outputs": [],
      "source": [
        "# ===== task specific functions =====\n",
        "from pykrx import stock\n",
        "import yfinance as yf\n",
        "\n",
        "# buy&sell signal\n",
        "def getBreakthroughPoint(df, col1, col2, patient_days, fill_method=\"fb\"):\n",
        "    '''\n",
        "    :param df: dataframe (including col1, col2)\n",
        "    :param col1: obj\n",
        "    :param col2: obj moving average\n",
        "    :param patient_days: patient days detected as breakthrough point\n",
        "    :return: signal series\n",
        "    '''\n",
        "    sigPrice = []\n",
        "    flag = -1  # A flag for the trend upward/downward\n",
        "\n",
        "    for i in range(0, len(df)):\n",
        "        if df[col1][i] > df[col2][i] and flag != 1:\n",
        "            tmp = df['Close'][i:(i + patient_days + 1)]\n",
        "            if len(tmp) == 1:\n",
        "                sigPrice.append(\"buy\")\n",
        "                flag = 1\n",
        "            else:\n",
        "                if (tmp.iloc[1:] > tmp.iloc[0]).all():\n",
        "                    sigPrice.append(\"buy\")\n",
        "                    flag = 1\n",
        "                else:\n",
        "                    sigPrice.append(nan)\n",
        "        elif df[col1][i] < df[col2][i] and flag != 0:\n",
        "            tmp = df['Close'][i:(i + patient_days + 1)]\n",
        "            if len(tmp) == 1:\n",
        "                sigPrice.append(\"sell\")\n",
        "                flag = 0\n",
        "            else:\n",
        "                if (tmp.iloc[1:] < tmp.iloc[0]).all():\n",
        "                    sigPrice.append(\"sell\")\n",
        "                    flag = 0\n",
        "                else:\n",
        "                    sigPrice.append(nan)\n",
        "        else:\n",
        "            sigPrice.append(nan)\n",
        "\n",
        "    sigPrice = series(sigPrice)\n",
        "    for idx, value in enumerate(sigPrice):\n",
        "        if not isna(value):\n",
        "            if value == \"buy\":\n",
        "                sigPrice.iloc[1:idx] = \"sell\"\n",
        "            else:\n",
        "                sigPrice.iloc[1:idx] = \"buy\"\n",
        "            break\n",
        "\n",
        "    sigPrice.ffill(inplace=True)\n",
        "    return sigPrice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5ctNjDjbegp"
      },
      "source": [
        "# **2. Load Raw Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9fktYc6fS0-M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== 삼성전자 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:08,  8.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== SK하이닉스 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2it [00:11,  4.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== NAVER =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [00:13,  3.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== 카카오 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4it [00:14,  2.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== 삼성바이오로직스 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:16,  2.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== 삼성전자우 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6it [00:18,  2.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== LG화학 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7it [00:20,  2.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== 삼성SDI =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8it [00:22,  2.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== 현대차 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "9it [00:24,  1.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== 셀트리온 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10it [00:25,  2.58s/it]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Colab Notebooks/dacon/closing price/\"\n",
        "\n",
        "# ====== raw data loading ======\n",
        "\n",
        "#1-- Get Stock List\n",
        "# 종목 코드 로드\n",
        "stock_list = read_csv(\"open/Stock_List.csv\")\n",
        "stock_list['종목코드'] = stock_list['종목코드'].apply(lambda x: str(x).zfill(6))\n",
        "\n",
        "\n",
        "#2-- 종목명을 key로 dict(=stock_dict) 생성\n",
        "stock_list.set_index(\"종목명\", inplace=True)\n",
        "selected_codes = stock_list.index.tolist()\n",
        "stock_list = stock_list.loc[selected_codes][\"종목코드\"]\n",
        "stock_dic = dict.fromkeys(selected_codes) \n",
        "\n",
        "\n",
        "#3-- 날짜 지정\n",
        "start_date = '20190101'; end_date = '20211126'  # 2019 01월 01일 ~ 2021년 11월 26일 데이터 load\n",
        "\n",
        "\n",
        "#4-- Original data loading\n",
        "for stock_name, stock_code in tqdm(stock_list.items()):\n",
        "    print(\"=====\", stock_name, \"=====\")\n",
        "    business_days = pd.DataFrame(pd.date_range(start_date, end_date, freq='B'), columns=['Date'])\n",
        "\n",
        "    # 종목 주가 데이터 로드\n",
        "    try:\n",
        "        stock_dic[stock_name] = dict.fromkeys([\"df\", \"target_list\"])       \n",
        "        # stock data\n",
        "        stock_df = stock.get_market_ohlcv_by_date(start_date, end_date, stock_code).reset_index()\n",
        "        # 기관합계&외국인 합계\n",
        "        investor_df = stock.get_market_trading_volume_by_date(start_date, end_date, stock_code)[[\"기관합계\", \"외국인합계\"]].reset_index()\n",
        "        # kospi 종가\n",
        "        kospi_df = stock.get_index_ohlcv_by_date(start_date, end_date, \"1001\")[[\"종가\"]].reset_index()\n",
        "\n",
        "        # columns name 변경\n",
        "        stock_df.columns = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "        investor_df.columns = [\"Date\", \"inst\", \"fore\"]\n",
        "        kospi_df.columns = [\"Date\", \"kospi\"]\n",
        "        business_days = business_days[business_days[\"Date\"] >= stock_df[\"Date\"][0]]\n",
        "\n",
        "        # 영업일과 주가 정보를 outer 조인\n",
        "        train_x = pd.merge(business_days, stock_df, how='left', on=\"Date\")\n",
        "        train_x = pd.merge(train_x, investor_df, how='left', on=\"Date\")\n",
        "        train_x = pd.merge(train_x, kospi_df, how='left', on=\"Date\")\n",
        "\n",
        "        # 앞의 일자로 nan값 forward fill\n",
        "        train_x.iloc[:, 1:] = train_x.iloc[:, 1:].ffill(axis=0)\n",
        "        # 첫 날이 na 일 가능성이 있으므로 backward fill 수행\n",
        "        train_x.iloc[:, 1:] = train_x.iloc[:, 1:].bfill(axis=0)\n",
        "\n",
        "    except:\n",
        "        # 기간&외국인 합계 없는 경우\n",
        "        stock_dic[stock_name] = dict.fromkeys([\"df\", \"target_list\"])\n",
        "        stock_df = stock.get_market_ohlcv_by_date(start_date, end_date, stock_code).reset_index()\n",
        "        kospi_df = stock.get_index_ohlcv_by_date(start_date, end_date, \"1001\")[[\"종가\"]].reset_index()\n",
        "\n",
        "        stock_df.columns = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "        kospi_df.columns = [\"Date\", \"kospi\"]\n",
        "        business_days = business_days[business_days[\"Date\"] >= stock_df[\"Date\"][0]]\n",
        "\n",
        "        # 영업일과 주가 정보를 outer 조인\n",
        "        train_x = pd.merge(business_days, stock_df, how='left', on=\"Date\")\n",
        "        train_x = pd.merge(train_x, kospi_df, how='left', on=\"Date\")\n",
        "\n",
        "        # 앞의 일자로 nan값 forward fill\n",
        "        train_x.iloc[:, 1:] = train_x.iloc[:, 1:].ffill(axis=0)\n",
        "        # 첫 날이 na 일 가능성이 있으므로 backward fill 수행\n",
        "        train_x.iloc[:, 1:] = train_x.iloc[:, 1:].bfill(axis=0)\n",
        "    stock_dic[stock_name][\"df\"] = train_x.copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          Date      Open      High       Low     Close     Volume      inst      fore    kospi\n",
            "0   2019-01-02  206671.0  208051.0  196084.0  197467.0  1135527.0 -140560.0  -13882.0  2010.00\n",
            "1   2019-01-03  197004.0  200227.0  191942.0  193785.0  1034002.0  -23088.0  -48775.0  1993.70\n",
            "2   2019-01-04  193783.0  203450.0  191482.0  202531.0  1229921.0   82112.0  -18115.0  2010.25\n",
            "3   2019-01-07  208973.0  208973.0  197926.0  198847.0  1164199.0  -73544.0 -168148.0  2037.10\n",
            "4   2019-01-08  199305.0  201608.0  195623.0  196546.0   702682.0   12412.0    9915.0  2025.27\n",
            "..         ...       ...       ...       ...       ...        ...       ...       ...      ...\n",
            "753 2021-11-22  216907.0  223287.0  214944.0  220833.0   576844.0  116332.0  -64748.0  3013.25\n",
            "754 2021-11-23  214944.0  216416.0  202675.0  207583.0  1453083.0 -165584.0 -228540.0  2997.33\n",
            "755 2021-11-24  206601.0  210527.0  204638.0  207093.0   684436.0 -110301.0    4207.0  2994.29\n",
            "756 2021-11-25  207583.0  210037.0  204638.0  205130.0   484671.0  -14241.0  -81501.0  2980.27\n",
            "757 2021-11-26  205129.0  214453.0  204148.0  210037.0   887177.0     421.0   81025.0  2936.44\n",
            "\n",
            "[758 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "print(train_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fp85yxZU0hTQ"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'easyIO' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22176\\3321958192.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# dataset dict를 pickle 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstock_dic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0measyIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"dataset/stock_df_ori_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mend_date\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'easyIO' is not defined"
          ]
        }
      ],
      "source": [
        "# dataset dict를 pickle 저장\n",
        "stock_dic = easyIO(None, folder_path + \"dataset/stock_df_ori_\" + start_date + \"_\" + end_date + \".pickle\", op=\"r\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TwpBANPDTI36"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'easyIO' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22176\\2827310259.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mend_date_yf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'2021-11-26'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstock_dic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0measyIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"dataset/stock_df_ori_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mend_date\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 해외index 불러오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'easyIO' is not defined"
          ]
        }
      ],
      "source": [
        "start_date_yf = '2019-01-01'\n",
        "end_date_yf = '2021-11-26'\n",
        "\n",
        "stock_dic = easyIO(None, folder_path + \"dataset/stock_df_ori_\" + start_date + \"_\" + end_date + \".pickle\", op=\"r\")\n",
        "\n",
        "# 해외index 불러오기\n",
        "forex_index_data = yf.download([\"USDKRW=X\", \"USDAUD=X\", \"USDJPY=X\", \"EURUSD=X\", \"CNY=X\", \"^GSPC\", \"^DJI\", \"^IXIC\", \"^STOXX50E\",\n",
        "                                \"^SOX\",  \"000001.SS\", \"000300.SS\", \"MME=F\", \"^TNX\"], start=start_date_yf, end=end_date_yf, rounding=True)\n",
        "\n",
        "tmp_forex_index = forex_index_data[\"Close\"]\n",
        "tmp_forex_index.index = pd.to_datetime(tmp_forex_index.index)\n",
        "tmp_forex_index = tmp_forex_index[(tmp_forex_index.index >= pd.to_datetime(start_date)) & (tmp_forex_index.index <= pd.to_datetime(end_date))]\n",
        "tmp_forex_index.columns = [\"sse_composite_index\", \"csi300_index\", \"usdtocny\", \"eurtousd\", \"msci_emerging\", \"usdtoaud\", \"usdtojpy\", \"usdtokrw\",\n",
        "                           \"dow\", \"snp500\", \"nasdaq\", \"semicon_index\", \"euro50\", \"us10y_tsy\"]\n",
        "\n",
        "tmp_forex_index.reset_index(drop=False, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anyLCJ1B2mso"
      },
      "source": [
        "# **3. Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiXoxpMCTQhR"
      },
      "outputs": [],
      "source": [
        "# ===== feature engineering =====\n",
        "non_stock = []\n",
        "corr_list = []\n",
        "timeunit_gap_forviz = 1\n",
        "metric_days = 14\n",
        "cat_vars = []\n",
        "bin_vars = []\n",
        "cat_vars.append(\"weekday\")\n",
        "cat_vars.append(\"weeknum\")\n",
        "bin_vars.append(\"mfi_signal\")\n",
        "num_pvalue_check = None\n",
        "cat_pvalue_check = series(0, index=[\"weekday\", \"weeknum\", \"mfi_signal\"])\n",
        "\n",
        "\n",
        "for stock_name, stock_data in stock_dic.items():\n",
        "    train_x = stock_data[\"df\"].copy()\n",
        "\n",
        "    # 1. 환율 및 관련 인덱스 feature 추가\n",
        "    train_x = pd.merge(train_x, tmp_forex_index, how=\"left\", on=\"Date\")\n",
        "    train_x = train_x.ffill() # 과거 일자로 forward fill 수행    \n",
        "    train_x = train_x.bfill() # 첫 날이 nan 일 가능성이 있으므로 backward fill 수행\n",
        "\n",
        "\n",
        "    # 2. 요일 및 주차 파생변수 추가\n",
        "    train_x['weekday'] = train_x[\"Date\"].apply(lambda x: x.weekday())\n",
        "    train_x['weeknum'] = train_x[\"Date\"].apply(lambda x: week_of_month(x))\n",
        "\n",
        "\n",
        "    # 3. 거래대금 파생변수 추가\n",
        "    train_x['trading_amount'] = train_x[\"Close\"] * train_x[\"Volume\"]\n",
        "\n",
        "    # 4. 월별 주기성 특징을 잡기 위한 sin 및 cos 변환 파생변수 추가\n",
        "    day_to_sec = 24 * 60 * 60\n",
        "    month_to_sec = 20 * day_to_sec\n",
        "    timestamp_s = train_x[\"Date\"].apply(datetime.timestamp)\n",
        "    timestamp_freq = round((timestamp_s / month_to_sec).diff(20)[20], 1)\n",
        "    train_x['dayofmonth_freq_sin'] = np.sin((timestamp_s / month_to_sec) * ((2 * np.pi) / timestamp_freq))\n",
        "    train_x['dayofmonth_freq_cos'] = np.cos((timestamp_s / month_to_sec) * ((2 * np.pi) / timestamp_freq))\n",
        "\n",
        "\n",
        "    # 5. OBV 파생변수 추가\n",
        "    # 매수 신호: obv > obv_ema\n",
        "    # 매도 신호: obv < obv_ema\n",
        "    obv = [0]\n",
        "    for i in range(1, len(train_x.Close)):\n",
        "        if train_x.Close[i] >= train_x.Close[i - 1]:\n",
        "            obv.append(obv[-1] + train_x.Volume[i])\n",
        "        elif train_x.Close[i] < train_x.Close[i - 1]:\n",
        "            obv.append(obv[-1] - train_x.Volume[i])\n",
        "    train_x['obv'] = obv\n",
        "    train_x['obv'][0] = nan\n",
        "    train_x['obv_ema'] = train_x['obv'].ewm(com=metric_days, min_periods=metric_days).mean()\n",
        "\n",
        "\n",
        "    # 6. Stochastic 파생변수 추가\n",
        "    # fast_d = moving average on fast_k\n",
        "    train_x[[\"fast_k\", \"fast_d\"]] = stochastic(train_x, n=metric_days)[[\"fast_k\", \"fast_d\"]]\n",
        "\n",
        "\n",
        "    # 7. MFI 파생변수 추가\n",
        "    # MFI = 100 - (100 / 1 + MFR)\n",
        "    # MFR = 14일간의 양의 MF / 14일간의 음의 MF\n",
        "    # MF = 거래량 * (당일고가 + 당일저가 + 당일종가) / 3\n",
        "    # MF 컬럼 만들기\n",
        "    train_x[\"mf\"] = train_x[\"Volume\"] * ((train_x[\"High\"]+train_x[\"Low\"]+train_x[\"Close\"]) / 3)\n",
        "    # 양의 MF와 음의 MF 표기 컬럼 만들기\n",
        "    p_n = []\n",
        "    for i in range(len(train_x['mf'])):\n",
        "        if i == 0 :\n",
        "            p_n.append(nan)\n",
        "        else:\n",
        "            if train_x['mf'][i] >= train_x['mf'][i-1]:\n",
        "                p_n.append('p')\n",
        "            else:\n",
        "                p_n.append('n')\n",
        "    train_x['p_n'] = p_n\n",
        "    # 14일간 양의 MF/ 14일간 음의 MF 계산하여 컬럼 만들기\n",
        "    mfr = []\n",
        "    for i in range(len(train_x['mf'])):\n",
        "        if i < metric_days-1:\n",
        "            mfr.append(nan)\n",
        "        else:\n",
        "            train_x_=train_x.iloc[(i-metric_days+1):i]\n",
        "            a = (sum(train_x_['mf'][train_x['p_n'] == 'p']) + 1) / (sum(train_x_['mf'][train_x['p_n'] == 'n']) + 10)\n",
        "            mfr.append(a)\n",
        "    train_x['mfr'] = mfr\n",
        "    # 최종 MFI 컬럼 만들기\n",
        "    train_x['mfi'] = 100 - (100 / (1 + train_x['mfr']))\n",
        "    train_x[\"mfi_signal\"] = train_x['mfi'].apply(lambda x: \"buy\" if x > 50 else \"sell\")\n",
        "\n",
        "\n",
        "    # 8. 이동평균 추가\n",
        "    train_x[\"close_mv5\"] = train_x[\"Close\"].rolling(5, min_periods=5).mean()\n",
        "    train_x[\"close_mv10\"] = train_x[\"Close\"].rolling(10, min_periods=10).mean()\n",
        "    train_x[\"close_mv20\"] = train_x[\"Close\"].rolling(20, min_periods=20).mean()\n",
        "\n",
        "    train_x[\"volume_mv5\"] = train_x[\"Volume\"].rolling(5, min_periods=5).mean()\n",
        "    train_x[\"volume_mv10\"] = train_x[\"Volume\"].rolling(10, min_periods=10).mean()\n",
        "    train_x[\"volume_mv20\"] = train_x[\"Volume\"].rolling(20, min_periods=20).mean()\n",
        "\n",
        "    train_x[\"trading_amount_mv5\"] = train_x[\"trading_amount\"].rolling(5, min_periods=5).mean()\n",
        "    train_x[\"trading_amount_mv10\"] = train_x[\"trading_amount\"].rolling(10, min_periods=10).mean()\n",
        "    train_x[\"trading_amount_mv20\"] = train_x[\"trading_amount\"].rolling(20, min_periods=20).mean()\n",
        "\n",
        "    train_x[\"kospi_mv5\"] = train_x[\"kospi\"].rolling(5, min_periods=5).mean()\n",
        "    train_x[\"kospi_mv10\"] = train_x[\"kospi\"].rolling(10, min_periods=10).mean()\n",
        "    train_x[\"kospi_mv20\"] = train_x[\"kospi\"].rolling(20, min_periods=20).mean()\n",
        "\n",
        "    try:\n",
        "        train_x[\"inst_mv5\"] = train_x[\"inst\"].rolling(5, min_periods=5).mean()\n",
        "        train_x[\"inst_mv10\"] = train_x[\"inst\"].rolling(10, min_periods=10).mean()\n",
        "        train_x[\"inst_mv20\"] = train_x[\"inst\"].rolling(20, min_periods=20).mean()\n",
        "\n",
        "        # 기관 연속 순매수 일자 feature 생성\n",
        "        cnt_consecutive = 0\n",
        "        tmp_consecutive = []\n",
        "        for i in train_x[\"inst\"]:\n",
        "            if i > 0:\n",
        "                cnt_consecutive += 1\n",
        "            else:\n",
        "                cnt_consecutive = 0\n",
        "            tmp_consecutive.append(cnt_consecutive)\n",
        "        train_x[\"consec_inst\"] = tmp_consecutive\n",
        "\n",
        "        train_x[\"fore_mv5\"] = train_x[\"fore\"].rolling(5, min_periods=5).mean()\n",
        "        train_x[\"fore_mv10\"] = train_x[\"fore\"].rolling(10, min_periods=10).mean()\n",
        "        train_x[\"fore_mv20\"] = train_x[\"fore\"].rolling(20, min_periods=20).mean()\n",
        "\n",
        "        # 외국인 연속 순매수 일자 feature 생성\n",
        "        cnt_consecutive = 0\n",
        "        tmp_consecutive = []\n",
        "        for i in train_x[\"fore\"]:\n",
        "            if i > 0:\n",
        "                cnt_consecutive += 1\n",
        "            else:\n",
        "                cnt_consecutive = 0\n",
        "            tmp_consecutive.append(cnt_consecutive)\n",
        "        train_x[\"consec_fore\"] = tmp_consecutive\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    # 9. 과거데이터 추가\n",
        "    tmp_df = dataframe()\n",
        "    tmp_cols = []\n",
        "    #1~5일 전 종가 load\n",
        "    for i in range(1,6,1):\n",
        "        tmp_df = pd.concat([tmp_df, train_x[\"Close\"].shift(i).to_frame()], axis=1)\n",
        "        tmp_cols.append(\"close_\" + str(i) + \"shift\")\n",
        "    tmp_df.columns = tmp_cols\n",
        "    train_x = pd.concat([train_x, tmp_df], axis=1)\n",
        "\n",
        "\n",
        "    # 10. columns 정리\n",
        "    # 지표계산을 위해 쓰인 컬럼 drop\n",
        "    train_x.drop([\"mf\", \"p_n\", \"mfr\"], inplace=True, axis=1)\n",
        "    # 컬럼이름 소문자 변환 및 정렬\n",
        "    train_x.columns = train_x.columns.str.lower()\n",
        "    train_x = pd.concat([train_x[[\"date\"]], train_x.iloc[:,1:].sort_index(axis=1)], axis=1)\n",
        "\n",
        "\n",
        "    # 11. create target list\n",
        "    target_list = []\n",
        "    target_list.append(train_x[\"close\"].copy())\n",
        "    target_list.append(train_x[\"close\"].shift(-1))\n",
        "    target_list.append(train_x[\"close\"].shift(-2))\n",
        "    target_list.append(train_x[\"close\"].shift(-3))\n",
        "    target_list.append(train_x[\"close\"].shift(-4))\n",
        "    target_list.append(train_x[\"close\"].shift(-5))\n",
        "    for idx, value in enumerate(target_list):\n",
        "        value.name = \"target_shift\" + str(idx)\n",
        "\n",
        "\n",
        "    # 12. onehot encoding & df save\n",
        "    onehot_encoder = MyOneHotEncoder()\n",
        "    train_x = onehot_encoder.fit_transform(train_x, cat_vars + bin_vars)\n",
        "    stock_dic[stock_name][\"df\"] = train_x.copy()\n",
        "    stock_dic[stock_name][\"target_list\"] = target_list\n",
        "\n",
        "\n",
        "easyIO(stock_dic, folder_path + \"dataset/stock_df_fe_\" + start_date + \"_\" + end_date + \".pickle\", op=\"w\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xXyqwYJT0uH"
      },
      "outputs": [],
      "source": [
        "# ===== feature selection and feature scaling =====\n",
        "\n",
        "# 1. Name of dummy variable colums\n",
        "cat_vars_oh = [\"weekday_0\", \"weekday_1\", \"weekday_2\", \"weekday_3\", \"weekday_4\",\n",
        "               \"weeknum_1\", \"weeknum_2\", \"weeknum_3\", \"weeknum_4\", \"weeknum_5\",\n",
        "               'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
        "               'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12']\n",
        "bin_vars_oh = [\"mfi_signal_buy\", \"mfi_signal_sell\"]\n",
        "forex_index_vars = [\"sse_composite_index\", \"csi300_index\", \"usdtocny\", \"eurtousd\", \"msci_emerging\",\n",
        "                    \"usdtoaud\", \"usdtojpy\", \"usdtokrw\", \"dow\", \"snp500\", \"nasdaq\", \"semicon_index\", \"euro50\", \"us10y_tsy\"]\n",
        "month_var = ['month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12']\n",
        "\n",
        "\n",
        "# 2. features 저장할 dict 생성\n",
        "feature_set_dic = dict.fromkeys(range(1,50))\n",
        "for i in feature_set_dic.keys(): feature_set_dic[i] = {}\n",
        "\n",
        "\n",
        "# # All features\n",
        "# feature_seed = 1\n",
        "# feature_set_dic[feature_seed][\"selected_features\"] = ['date', 'close', 'close_1shift', 'close_2shift', 'close_3shift', 'close_4shift', 'close_5shift', 'close_mv10', 'close_mv20', 'close_mv5',\n",
        "#                      'consec_fore', 'consec_inst', 'csi300_index', 'dayofmonth_freq_cos', 'dayofmonth_freq_sin', 'dow', 'euro50', 'eurtousd', 'fast_d', 'fast_k',\n",
        "#                      'fore', 'fore_mv10', 'fore_mv20', 'fore_mv5', 'inst', 'inst_mv10', 'inst_mv20', 'inst_mv5', 'kospi', 'kospi_mv10', 'kospi_mv20', 'kospi_mv5',\n",
        "#                      'mfi', 'mfi_signal_buy', 'mfi_signal_sell', 'msci_emerging', 'nasdaq', 'obv', 'obv_ema', 'semicon_index', 'snp500', 'sse_composite_index',\n",
        "#                      'trading_amount', 'trading_amount_mv10', 'trading_amount_mv20', 'trading_amount_mv5', 'us10y_tsy', 'usdtoaud', 'usdtocny', 'usdtojpy', 'usdtokrw',\n",
        "#                      'volume', 'volume_mv10', 'volume_mv20', 'volume_mv5', 'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\n",
        "#                      'weeknum_1', 'weeknum_2', 'weeknum_3', 'weeknum_4', 'weeknum_5']\n",
        "# feature_set_dic[feature_seed][\"logtrans_vec\"] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m09R__dRUC42"
      },
      "outputs": [],
      "source": [
        "# 2. features 49가지 조합 중 가장 좋은 성능을 가지는 seed 7 load \n",
        "\n",
        "feature_seed = 7\n",
        "# feature_set_dic[7][\"selected_features\"] = [\"date\", \"close\"] + [\"weekday_0\", \"weekday_1\", \"weekday_2\", \"weekday_3\", \"weekday_4\"]\n",
        "\n",
        "feature_set_dic = easyIO(None, folder_path + \"dataset/feature_dic.pkl\", op=\"r\")\n",
        "selected_features = feature_set_dic[feature_seed][\"selected_features\"]\n",
        "logtrans_vec = feature_set_dic[feature_seed][\"logtrans_vec\"]\n",
        "\n",
        "feature_name = \"feature_seed_\" + str(feature_seed)\n",
        "createFolder(folder_path + \"result/\" + feature_name + \"/\")\n",
        "createFolder(folder_path + \"submission/\" + feature_name + \"/\")\n",
        "\n",
        "# date cutoff config\n",
        "date_cutoff = None\n",
        "# anomaly cutoff config\n",
        "anomaly_cutoff = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a30MJxqqVAMy"
      },
      "outputs": [],
      "source": [
        "# 3. 가격 데이터를 활용한 features & Scaling\n",
        "\n",
        "for stock_name, stock_data in tqdm(stock_dic.items()):\n",
        "    train_x = stock_data[\"df\"].copy()\n",
        "\n",
        "    train_x[\"month\"] = train_x[\"date\"].dt.month\n",
        "    oh_encoder = MyOneHotEncoder()\n",
        "    train_x = oh_encoder.fit_transform(train_x, [\"month\"])\n",
        "\n",
        "    train_x[\"close_return\"] = train_x[\"close\"].pct_change()\n",
        "    train_x[\"std20\"] = train_x[\"close\"].rolling(20).std()\n",
        "\n",
        "    train_x[\"close_mean_2\"] = (train_x[\"high\"] + train_x[\"low\"]) / 2\n",
        "    train_x[\"close_mean_3\"] = (train_x[\"high\"] + train_x[\"low\"] + train_x[\"close\"]) / 3\n",
        "    train_x[\"close_diff\"] = train_x[\"close_mean_3\"] - train_x[\"close_mean_2\"]\n",
        "    train_x[\"high_low_range\"] = train_x[\"high\"] - train_x[\"low\"]\n",
        "    train_x[\"high_low_range_return\"] = train_x[\"high_low_range\"].pct_change()\n",
        "\n",
        "    train_x[\"high_to_close\"] = train_x[\"high\"] - train_x[\"close\"]\n",
        "    train_x[\"close_to_low\"] = train_x[\"close\"] - train_x[\"low\"]\n",
        "\n",
        "    train_x[\"snp500_return\"] = train_x[\"snp500\"].pct_change()\n",
        "    train_x[\"dow_return\"] = train_x[\"dow\"].pct_change()\n",
        "    train_x[\"nasdaq_return\"] = train_x[\"nasdaq\"].pct_change()\n",
        "    train_x[\"semicon_index_return\"] = train_x[\"semicon_index\"].pct_change()\n",
        "\n",
        "    train_x[\"sse_composite_index_return\"] = train_x[\"sse_composite_index\"].pct_change()\n",
        "    train_x[\"usdtokrw_return\"] = train_x[\"usdtokrw\"].pct_change()\n",
        "    train_x[\"eurtousd_return\"] = train_x[\"eurtousd\"].pct_change()\n",
        "    train_x[\"us10y_tsy_return\"] = train_x[\"us10y_tsy\"].pct_change()\n",
        "\n",
        "    train_x.replace(np.inf, 0, inplace=True)\n",
        "    train_x.replace(-np.inf, 0, inplace=True)\n",
        "\n",
        "    # 3.1 smoothing on target\n",
        "    train_x[\"close_smoothing\"] = 1\n",
        "    if anomaly_cutoff is not None:\n",
        "        tmp_list = []\n",
        "        for idx, value in enumerate(train_x[\"close_return\"]):\n",
        "            if isna(value):\n",
        "                tmp_list.append(train_x[\"close\"].iloc[idx])\n",
        "                continue\n",
        "            if np.abs(value) > anomaly_cutoff:\n",
        "                print(\"anomaly detected :\", stock_name)\n",
        "                if value >= 0:\n",
        "                    tmp_list.append(train_x[\"close\"].iloc[idx - 1] * (1 + anomaly_cutoff))\n",
        "                else:\n",
        "                    tmp_list.append(train_x[\"close\"].iloc[idx - 1] * (1 - anomaly_cutoff))\n",
        "            else:\n",
        "                tmp_list.append(train_x[\"close\"].iloc[idx])\n",
        "        train_x[\"close_smoothing\"] = series(tmp_list)\n",
        "    train_x.ffill(inplace=True)\n",
        "\n",
        "    # 3.2 <<feature selection>> ??\n",
        "    if selected_features is not None:\n",
        "        tmp_list = [i for i in selected_features if i in train_x.columns] + [\"close_smoothing\"]\n",
        "        if len(tmp_list) > 1:\n",
        "            train_x = train_x[tmp_list]\n",
        "\n",
        "    if date_cutoff is not None:\n",
        "        train_x = train_x[train_x[\"date\"] >= date_cutoff]\n",
        "\n",
        "    train_x = train_x.dropna()\n",
        "    train_x.reset_index(drop=True, inplace=True)\n",
        "    tmp_target = train_x[\"close_smoothing\"] if anomaly_cutoff is not None else train_x[\"close\"]\n",
        "    train_x.drop(\"close_smoothing\", axis=1, inplace=True)\n",
        "\n",
        "\n",
        "    # 3.3 과거데이터 추가\n",
        "    tmp_df = dataframe()\n",
        "    tmp_cols = []\n",
        "    for i in range(1, 6, 1):\n",
        "        tmp_df = pd.concat([tmp_df, stock_data[\"df\"][\"close\"].shift(i).to_frame()], axis=1)\n",
        "        tmp_cols.append(\"close_\" + str(i) + \"shift\")\n",
        "    tmp_df.columns = tmp_cols\n",
        "    stock_data[\"df\"] = pd.concat([stock_data[\"df\"], tmp_df], axis=1)\n",
        "\n",
        "\n",
        "    # 3.4 create target list\n",
        "    target_list = []\n",
        "    target_list.append(tmp_target.copy())\n",
        "    target_list.append(tmp_target.shift(-1))\n",
        "    target_list.append(tmp_target.shift(-2))\n",
        "    target_list.append(tmp_target.shift(-3))\n",
        "    target_list.append(tmp_target.shift(-4))\n",
        "    target_list.append(tmp_target.shift(-5))\n",
        "    for idx, value in enumerate(target_list):\n",
        "        value.name = \"target_shift\" + str(idx)\n",
        "\n",
        "    for i in train_x.columns[1:]:\n",
        "        train_x[i] = train_x[i].astype(\"float32\")\n",
        "        if len(np.where(np.isinf(train_x[i]))[0]) > 0:\n",
        "            train_x[i][np.where(np.isinf(train_x[i]))[0]] = nan\n",
        "            train_x[i].ffill(inplace=True)\n",
        "\n",
        "\n",
        "    # 3.5 <feature scaling>\n",
        "    # log transform\n",
        "    for i in logtrans_vec:\n",
        "        if i in train_x.columns:\n",
        "            train_x[i] = train_x[i].apply(np.log1p)\n",
        "\n",
        "    stock_dic[stock_name][\"df\"] = train_x.copy()\n",
        "    stock_dic[stock_name][\"target_list\"] = target_list\n",
        "    del train_x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wXfeRanFKbx"
      },
      "source": [
        "# **4. Predict with Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYcPG2OsVayU"
      },
      "outputs": [],
      "source": [
        "# 학습 전 필요 변수 초기화\n",
        "kfolds_spliter = TimeSeriesSplit(n_splits=5, test_size=1, gap=0)\n",
        "\n",
        "targetType = \"numeric\"\n",
        "targetTask = None\n",
        "class_levels = [0, 1]\n",
        "cut_off = 0\n",
        "\n",
        "ds = None\n",
        "result_val = None\n",
        "result_test = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLdLSQsoKEj6"
      },
      "outputs": [],
      "source": [
        "#Model 평가 결과 Linear가 예측성능이 가장 높다\n",
        "# model_names = [\"Linear\", \"KNN\", \"MLP_Desc_V2\", \"MLP_ResNet_V1\"]\n",
        "model_names = [\"Linear\"]\n",
        "\n",
        "\n",
        "fit_runningtime = time()\n",
        "\n",
        "# 1. 데이터를 저장할 변수 설정\n",
        "total_perf = None\n",
        "for stock_name, stock_data in stock_dic.items():\n",
        "    stock_data[\"perf_list\"] = dict.fromkeys(model_names)\n",
        "    stock_data[\"pred_list\"] = dict.fromkeys(model_names)\n",
        "    stock_data[\"pred_list\"][\"best_pred\"] = dict.fromkeys([1, 2, 3, 4, 5], [0])\n",
        "    total_perf = dict.fromkeys(model_names)\n",
        "    for i in model_names:\n",
        "        stock_data[\"perf_list\"][i] = dict.fromkeys([1, 2, 3, 4, 5], [0])\n",
        "        stock_data[\"pred_list\"][i] = dict.fromkeys([1, 2, 3, 4, 5], [0])\n",
        "        total_perf[i] = dict.fromkeys([1, 2, 3, 4, 5], [0])\n",
        "        for j in total_perf[i].keys():\n",
        "            total_perf[i][j] = series(0, index=[\"MAE\", \"MAPE\", \"NMAE\", \"RMSE\", \"NRMSE\", \"R2\", \"Running_Time\"])\n",
        "\n",
        "target_timegap = 5\n",
        "val_size = 5\n",
        "seqLength = 5\n",
        "val_year = 2021; val_month = 11; val_day = 19\n",
        "test_year = 2021; test_month = 11; test_day = 26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmnMFV2qV0IL"
      },
      "outputs": [],
      "source": [
        "import pandas_datareader as pdr\n",
        "\n",
        "# 2. Model fit&predict\n",
        "for time_ngap in range(1,target_timegap+1):\n",
        "    print(F\"=== Target on N+{time_ngap} ===\")\n",
        "    # time_ngap = 1\n",
        "\n",
        "    # USD/JPY adjustment - 안전자산 선호심리 약,강 -> 급락,급등 파악\n",
        "    start_date_yf = \"/\".join([str(val_month), str(val_day - 5), str(val_year)])\n",
        "    end_date_yf = \"/\".join([str(val_month), str(val_day), str(val_year)])\n",
        "    tmp_usdjpy = pdr.DataReader(\"DEXJPUS\", \"fred\", start=start_date_yf, end=end_date_yf)\n",
        "    tmp_usdjpy.pct_change()\n",
        "    val_adj_usdjpy = round(tmp_usdjpy.ffill().pct_change().iloc[-1], 2)[0]\n",
        "\n",
        "    start_date_yf = \"/\".join([str(test_month), str(test_day - 5), str(test_year)])\n",
        "    end_date_yf = \"/\".join([str(test_month) ,str(test_day), str(test_year)])\n",
        "    tmp_usdjpy = pdr.DataReader(\"DEXJPUS\", \"fred\", start=start_date_yf, end=end_date_yf)\n",
        "    test_adj_usdjpy = round(tmp_usdjpy.ffill().pct_change().iloc[-1], 2)[0]\n",
        "\n",
        "    for stock_name, stock_data in tqdm(stock_dic.items()):\n",
        "\n",
        "        tmp_x = stock_data[\"df\"].copy()\n",
        "        tmp_y = copy.deepcopy(stock_data[\"target_list\"][time_ngap])\n",
        "        tmp_date = tmp_x[\"date\"]\n",
        "        arima_target = stock_data[\"target_list\"][0]\n",
        "        arima_date = stock_data[\"df\"][\"date\"]\n",
        "        tmp_x.drop(\"date\", axis=1, inplace=True)\n",
        "\n",
        "        # <선형회귀>\n",
        "        if \"Linear\" in model_names:\n",
        "            if time_ngap in [1, 2, 3, 4, 5]:\n",
        "                tmp_runtime = time()\n",
        "                print(\"Linear Regression on\", stock_name, \"\\n\")\n",
        "                # evaludation on validation set\n",
        "                numericCols = [i for i in tmp_x.columns if i not in cat_vars_oh + bin_vars_oh]\n",
        "                scaler_feature = StandardScaler()\n",
        "                scaler_target = StandardScaler()\n",
        "\n",
        "                train_x = tmp_x[tmp_date <= datetime(val_year, val_month, val_day)][:-val_size][:-time_ngap]\n",
        "                train_y = tmp_y[tmp_date <= datetime(val_year, val_month, val_day)][:-val_size][:-time_ngap]\n",
        "\n",
        "                tmp_anomaly = (train_y - train_x[\"close\"]) / train_x[\"close\"]\n",
        "                tmp_new_target = []\n",
        "                anomaly_cutoff = 0.1 * (1 + 0.2 * time_ngap)\n",
        "                for idx, value in enumerate(tmp_anomaly):\n",
        "                    if np.abs(value) > anomaly_cutoff:\n",
        "                        if value >= 0:\n",
        "                            tmp_new_target.append(train_x[\"close\"].iloc[idx] * (1 + anomaly_cutoff))\n",
        "                        else:\n",
        "                            tmp_new_target.append(train_x[\"close\"].iloc[idx] * (1 - anomaly_cutoff))\n",
        "                    else:\n",
        "                        tmp_new_target.append(train_y.iloc[idx])\n",
        "                train_y = tmp_new_target\n",
        "\n",
        "                val_x = tmp_x[tmp_date <= datetime(val_year, val_month, val_day)][-val_size:]\n",
        "                val_y = tmp_y[tmp_date <= datetime(val_year, val_month, val_day)][-val_size:]\n",
        "\n",
        "                # scaling\n",
        "                # scaler_target.fit(train_x[\"close\"].to_frame())\n",
        "                # train_y = series(scaler_target.transform(train_y.to_frame())[:, 0])\n",
        "                train_x[numericCols] = dataframe(scaler_feature.fit_transform(train_x[numericCols]), index=train_x.index, columns=numericCols)\n",
        "                val_x[numericCols] = dataframe(scaler_feature.transform(val_x[numericCols]), index=val_x.index, columns=numericCols)\n",
        "\n",
        "                model = lm.LinearRegression()\n",
        "                model.fit(train_x, train_y)\n",
        "\n",
        "                # # no adjustment\n",
        "                # tmp_pred = model.predict(val_x)[:, np.newaxis].flatten()\n",
        "\n",
        "                # adjustment\n",
        "                if abs(val_adj_usdjpy) != 0:\n",
        "                    tmp_pred = model.predict(val_x)[:, np.newaxis].flatten() * (1 + val_adj_usdjpy)\n",
        "                else:\n",
        "                    tmp_pred = model.predict(val_x)[:, np.newaxis].flatten()\n",
        "\n",
        "                mae = metrics.mean_absolute_error(val_y, tmp_pred)\n",
        "                rmse = metrics.mean_squared_error(val_y, tmp_pred, squared=False)\n",
        "                stock_data[\"perf_list\"][\"Linear\"][time_ngap] = {\"MAE\": mae,\n",
        "                                                                \"MAPE\": metrics.mean_absolute_percentage_error(val_y, tmp_pred),\n",
        "                                                                \"NMAE\": (mae / val_y.abs().mean()),\n",
        "                                                                \"RMSE\": rmse,\n",
        "                                                                \"NRMSE\": (rmse / val_y.abs().mean()),\n",
        "                                                                \"R2\": metrics.r2_score(val_y, tmp_pred)}\n",
        "                tmp_perf = series(stock_data[\"perf_list\"][\"Linear\"][time_ngap])\n",
        "                print(tmp_perf)\n",
        "\n",
        "                # prediction on test set\n",
        "                scaler_feature = StandardScaler()\n",
        "                scaler_target = StandardScaler()\n",
        "\n",
        "                full_x = tmp_x[tmp_date <= datetime(test_year, test_month, test_day)][:-time_ngap]\n",
        "                full_y = tmp_y[tmp_date <= datetime(test_year, test_month, test_day)][:-time_ngap]\n",
        "\n",
        "                tmp_anomaly = (full_y - full_x[\"close\"]) / full_x[\"close\"]\n",
        "                tmp_new_target = []\n",
        "                anomaly_cutoff = 0.1 * (1 + 0.2 * time_ngap)\n",
        "                for idx, value in enumerate(tmp_anomaly):\n",
        "                    if np.abs(value) > anomaly_cutoff:\n",
        "                        if value >= 0:\n",
        "                            tmp_new_target.append(full_x[\"close\"].iloc[idx] * (1 + anomaly_cutoff))\n",
        "                        else:\n",
        "                            tmp_new_target.append(full_x[\"close\"].iloc[idx] * (1 - anomaly_cutoff))\n",
        "                    else:\n",
        "                        tmp_new_target.append(full_y.iloc[idx])\n",
        "                full_y = tmp_new_target\n",
        "\n",
        "                test_x = tmp_x[tmp_date == datetime(test_year, test_month, test_day)]\n",
        "\n",
        "                # scaling\n",
        "                # scaler_target.fit(full_x[\"close\"].to_frame())\n",
        "                # full_y = series(scaler_target.transform(full_y.to_frame())[:, 0])\n",
        "                full_x[numericCols] = dataframe(scaler_feature.fit_transform(full_x[numericCols]), index=full_x.index, columns=numericCols)\n",
        "                test_x[numericCols] = dataframe(scaler_feature.transform(test_x[numericCols]), index=test_x.index, columns=numericCols)\n",
        "\n",
        "                model = lm.LinearRegression()\n",
        "                model.fit(full_x, full_y)\n",
        "                # stock_data[\"pred_list\"][\"Linear\"][time_ngap] = scaler_target.inverse_transform(model.predict(test_x)[:, np.newaxis]).flatten()\n",
        "                stock_data[\"pred_list\"][\"Linear\"][time_ngap] = model.predict(test_x)[:, np.newaxis].flatten()\n",
        "                tmp_runtime = time() - tmp_runtime\n",
        "                total_perf[\"Linear\"][time_ngap] += tmp_perf.append(series({\"Running_Time\": tmp_runtime}))\n",
        "                print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "    for i in model_names:\n",
        "        total_perf[i][time_ngap] /= len(stock_dic.keys())\n",
        "fit_runningtime = time() - fit_runningtime\n",
        "\n",
        "# prediction value check\n",
        "print(stock_dic[\"삼성전자\"][\"pred_list\"])\n",
        "print(fit_runningtime)                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKiC9xhYYhGC"
      },
      "outputs": [],
      "source": [
        "# 3. 성능평가 테이블 생성\n",
        "perf_table = dataframe(index=model_names, columns=[\"time_gap_\" + str(i) for i in range(1,6)])\n",
        "runningtime_table = dataframe(index=model_names, columns=[\"time_gap_\" + str(i) for i in range(1,6)])\n",
        "for i in list(total_perf.keys()):\n",
        "    if array(list(total_perf[i].values())).sum() == 0:\n",
        "        pass\n",
        "    else:\n",
        "        perf_table.loc[i] = dataframe(total_perf[i]).loc[\"NMAE\"].values\n",
        "        runningtime_table.loc[i] = dataframe(total_perf[i]).loc[\"Running_Time\"].values\n",
        "\n",
        "# NMAE = MAPE\n",
        "perf_table = perf_table.iloc[:,:target_timegap]\n",
        "perf_table = perf_table * 100\n",
        "perf_table.loc[\"best_model\"] = perf_table.min(axis=0)\n",
        "perf_table[\"avg\"] = perf_table.iloc[:,:5].mean(axis=1)\n",
        "perf_table[\"std\"] = perf_table.iloc[:,:5].std(axis=1)\n",
        "perf_table[\"running_time\"] = runningtime_table.mean(axis=1).append(series({\"best_model\": -1}))\n",
        "print(perf_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD8fhhxYYsjX"
      },
      "outputs": [],
      "source": [
        "# 4. export submission file\n",
        "submission = read_csv(\"projects/dacon_stockprediction/sample_submission.csv\")\n",
        "for for_model in model_names:\n",
        "    for i in submission.columns[1:]:\n",
        "        tmp_list = []\n",
        "        for j in stock_dic[stock_list.index[stock_list == i][0]][\"pred_list\"][for_model].values():\n",
        "            tmp_list.append(j[0])\n",
        "        submission[i][:5] = tmp_list\n",
        "    submission.to_csv(folder_path + \"submission/\" + feature_name + \"/\" + feature_name + \"_\" + for_model + \".csv\", index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Dacon_Linear_fin",
      "provenance": []
    },
    "interpreter": {
      "hash": "020eb92898ac559bd98722b535e6235e035f297f752a3a61b26b02297b428980"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
